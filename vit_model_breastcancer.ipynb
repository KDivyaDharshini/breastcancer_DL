{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gwds_gV6iAI1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import cv2\n",
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import torchvision.models as m\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import random_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wg-DWMoYiPep",
        "outputId": "32327e7a-fc4a-4ba7-bf0b-1e7ab9dceca0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir=\"/content/drive/MyDrive/Dataset_BUSI_with_GT\"\n",
        "class_names=[]\n",
        "for path in os.listdir(base_dir):\n",
        "  class_names.append(path)\n",
        "\n",
        "print(class_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-diB7N_ij5g",
        "outputId": "36d21c10-edf4-4124-f84e-aab971a9b20c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['benign', 'normal', 'malignant']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=16\n",
        "hidden_size=256\n",
        "epoch_no=50\n"
      ],
      "metadata": {
        "id": "76PC6ZpJo6Ey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, base_dir, class_names, transform=None,max_samples_per_class=None):\n",
        "    self.base_dir = base_dir\n",
        "    self.class_names = class_names\n",
        "    self.transform = transform\n",
        "    self.image_paths = []\n",
        "    self.mask_paths = []\n",
        "    self.labels = []\n",
        "    self.max_samples_per_class = max_samples_per_class\n",
        "\n",
        "\n",
        "  def load_data(self):\n",
        "    class_samples = {class_name: 0 for class_name in self.class_names}\n",
        "    for class_name in self.class_names:\n",
        "        class_dir = os.path.join(self.base_dir, class_name)\n",
        "        image_files = [f for f in os.listdir(class_dir) if f.endswith('.png') and '_mask' not in f]\n",
        "        for image_file in image_files:\n",
        "            if class_samples[class_name] < self.max_samples_per_class:\n",
        "                mask_file = f\"{os.path.splitext(image_file)[0]}_mask.png\"\n",
        "                if os.path.exists(os.path.join(class_dir, mask_file)):\n",
        "                    self.image_paths.append(os.path.join(class_dir, image_file))\n",
        "                    self.mask_paths.append(os.path.join(class_dir, mask_file))\n",
        "                    label = self.class_names.index(class_name)\n",
        "                    self.labels.append(label)\n",
        "                    class_samples[class_name] += 1\n",
        "  def get_data(self):\n",
        "    return self.image_paths,self.mask_paths,self.labels\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.image_paths)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "        image_paths = self.image_paths[idx]\n",
        "        mask_paths = self.mask_paths[idx]\n",
        "        label=self.labels[idx]\n",
        "        image = cv2.imread(image_paths)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        mask = cv2.imread(mask_paths, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "            mask = mask_transform(mask)\n",
        "\n",
        "        label_tensor=torch.tensor(label)\n",
        "\n",
        "        return image, mask ,label_tensor"
      ],
      "metadata": {
        "id": "J5lnqNTf6r4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform=transforms.Compose([transforms.ToTensor(),transforms.Resize([224,224]),transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "mask_transform = transforms.Compose([transforms.ToTensor(),transforms.Resize([224,224])])\n",
        "\n",
        "dataset=Dataset(base_dir,class_names=class_names,transform=transform,max_samples_per_class=300)\n",
        "dataset.load_data()\n",
        "data_loader=torch.utils.data.DataLoader(dataset,batch_size=16,shuffle=False)\n"
      ],
      "metadata": {
        "id": "Er82FVPChiyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_len=len(dataset)\n",
        "train_size = int(0.8 * dataset_len)\n",
        "val_size = dataset_len - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader=torch.utils.data.DataLoader(train_dataset,batch_size=16,shuffle=False)\n",
        "val_loader=torch.utils.data.DataLoader(val_dataset,batch_size=16,shuffle=False)"
      ],
      "metadata": {
        "id": "rlAqWkT27how"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models.vision_transformer import VisionTransformer\n",
        "from torchvision.models import vit_b_16, ViT_B_16_Weights"
      ],
      "metadata": {
        "id": "ikSpitYmJScz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit_model = m.vit_b_16(weights=m.ViT_B_16_Weights.DEFAULT)\n",
        "for name, module in vit_model.named_modules():\n",
        "    print(f\"{name}: {module.__class__.__name__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "aYMlt4qwTVH8",
        "outputId": "ddd70341-a5e8-4820-8b57-50280c7ce357"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n",
            "100%|██████████| 330M/330M [00:02<00:00, 141MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ": VisionTransformer\n",
            "conv_proj: Conv2d\n",
            "encoder: Encoder\n",
            "encoder.dropout: Dropout\n",
            "encoder.layers: Sequential\n",
            "encoder.layers.encoder_layer_0: EncoderBlock\n",
            "encoder.layers.encoder_layer_0.ln_1: LayerNorm\n",
            "encoder.layers.encoder_layer_0.self_attention: MultiheadAttention\n",
            "encoder.layers.encoder_layer_0.self_attention.out_proj: NonDynamicallyQuantizableLinear\n",
            "encoder.layers.encoder_layer_0.dropout: Dropout\n",
            "encoder.layers.encoder_layer_0.ln_2: LayerNorm\n",
            "encoder.layers.encoder_layer_0.mlp: MLPBlock\n",
            "encoder.layers.encoder_layer_0.mlp.0: Linear\n",
            "encoder.layers.encoder_layer_0.mlp.1: GELU\n",
            "encoder.layers.encoder_layer_0.mlp.2: Dropout\n",
            "encoder.layers.encoder_layer_0.mlp.3: Linear\n",
            "encoder.layers.encoder_layer_0.mlp.4: Dropout\n",
            "encoder.layers.encoder_layer_1: EncoderBlock\n",
            "encoder.layers.encoder_layer_1.ln_1: LayerNorm\n",
            "encoder.layers.encoder_layer_1.self_attention: MultiheadAttention\n",
            "encoder.layers.encoder_layer_1.self_attention.out_proj: NonDynamicallyQuantizableLinear\n",
            "encoder.layers.encoder_layer_1.dropout: Dropout\n",
            "encoder.layers.encoder_layer_1.ln_2: LayerNorm\n",
            "encoder.layers.encoder_layer_1.mlp: MLPBlock\n",
            "encoder.layers.encoder_layer_1.mlp.0: Linear\n",
            "encoder.layers.encoder_layer_1.mlp.1: GELU\n",
            "encoder.layers.encoder_layer_1.mlp.2: Dropout\n",
            "encoder.layers.encoder_layer_1.mlp.3: Linear\n",
            "encoder.layers.encoder_layer_1.mlp.4: Dropout\n",
            "encoder.layers.encoder_layer_2: EncoderBlock\n",
            "encoder.layers.encoder_layer_2.ln_1: LayerNorm\n",
            "encoder.layers.encoder_layer_2.self_attention: MultiheadAttention\n",
            "encoder.layers.encoder_layer_2.self_attention.out_proj: NonDynamicallyQuantizableLinear\n",
            "encoder.layers.encoder_layer_2.dropout: Dropout\n",
            "encoder.layers.encoder_layer_2.ln_2: LayerNorm\n",
            "encoder.layers.encoder_layer_2.mlp: MLPBlock\n",
            "encoder.layers.encoder_layer_2.mlp.0: Linear\n",
            "encoder.layers.encoder_layer_2.mlp.1: GELU\n",
            "encoder.layers.encoder_layer_2.mlp.2: Dropout\n",
            "encoder.layers.encoder_layer_2.mlp.3: Linear\n",
            "encoder.layers.encoder_layer_2.mlp.4: Dropout\n",
            "encoder.layers.encoder_layer_3: EncoderBlock\n",
            "encoder.layers.encoder_layer_3.ln_1: LayerNorm\n",
            "encoder.layers.encoder_layer_3.self_attention: MultiheadAttention\n",
            "encoder.layers.encoder_layer_3.self_attention.out_proj: NonDynamicallyQuantizableLinear\n",
            "encoder.layers.encoder_layer_3.dropout: Dropout\n",
            "encoder.layers.encoder_layer_3.ln_2: LayerNorm\n",
            "encoder.layers.encoder_layer_3.mlp: MLPBlock\n",
            "encoder.layers.encoder_layer_3.mlp.0: Linear\n",
            "encoder.layers.encoder_layer_3.mlp.1: GELU\n",
            "encoder.layers.encoder_layer_3.mlp.2: Dropout\n",
            "encoder.layers.encoder_layer_3.mlp.3: Linear\n",
            "encoder.layers.encoder_layer_3.mlp.4: Dropout\n",
            "encoder.layers.encoder_layer_4: EncoderBlock\n",
            "encoder.layers.encoder_layer_4.ln_1: LayerNorm\n",
            "encoder.layers.encoder_layer_4.self_attention: MultiheadAttention\n",
            "encoder.layers.encoder_layer_4.self_attention.out_proj: NonDynamicallyQuantizableLinear\n",
            "encoder.layers.encoder_layer_4.dropout: Dropout\n",
            "encoder.layers.encoder_layer_4.ln_2: LayerNorm\n",
            "encoder.layers.encoder_layer_4.mlp: MLPBlock\n",
            "encoder.layers.encoder_layer_4.mlp.0: Linear\n",
            "encoder.layers.encoder_layer_4.mlp.1: GELU\n",
            "encoder.layers.encoder_layer_4.mlp.2: Dropout\n",
            "encoder.layers.encoder_layer_4.mlp.3: Linear\n",
            "encoder.layers.encoder_layer_4.mlp.4: Dropout\n",
            "encoder.layers.encoder_layer_5: EncoderBlock\n",
            "encoder.layers.encoder_layer_5.ln_1: LayerNorm\n",
            "encoder.layers.encoder_layer_5.self_attention: MultiheadAttention\n",
            "encoder.layers.encoder_layer_5.self_attention.out_proj: NonDynamicallyQuantizableLinear\n",
            "encoder.layers.encoder_layer_5.dropout: Dropout\n",
            "encoder.layers.encoder_layer_5.ln_2: LayerNorm\n",
            "encoder.layers.encoder_layer_5.mlp: MLPBlock\n",
            "encoder.layers.encoder_layer_5.mlp.0: Linear\n",
            "encoder.layers.encoder_layer_5.mlp.1: GELU\n",
            "encoder.layers.encoder_layer_5.mlp.2: Dropout\n",
            "encoder.layers.encoder_layer_5.mlp.3: Linear\n",
            "encoder.layers.encoder_layer_5.mlp.4: Dropout\n",
            "encoder.layers.encoder_layer_6: EncoderBlock\n",
            "encoder.layers.encoder_layer_6.ln_1: LayerNorm\n",
            "encoder.layers.encoder_layer_6.self_attention: MultiheadAttention\n",
            "encoder.layers.encoder_layer_6.self_attention.out_proj: NonDynamicallyQuantizableLinear\n",
            "encoder.layers.encoder_layer_6.dropout: Dropout\n",
            "encoder.layers.encoder_layer_6.ln_2: LayerNorm\n",
            "encoder.layers.encoder_layer_6.mlp: MLPBlock\n",
            "encoder.layers.encoder_layer_6.mlp.0: Linear\n",
            "encoder.layers.encoder_layer_6.mlp.1: GELU\n",
            "encoder.layers.encoder_layer_6.mlp.2: Dropout\n",
            "encoder.layers.encoder_layer_6.mlp.3: Linear\n",
            "encoder.layers.encoder_layer_6.mlp.4: Dropout\n",
            "encoder.layers.encoder_layer_7: EncoderBlock\n",
            "encoder.layers.encoder_layer_7.ln_1: LayerNorm\n",
            "encoder.layers.encoder_layer_7.self_attention: MultiheadAttention\n",
            "encoder.layers.encoder_layer_7.self_attention.out_proj: NonDynamicallyQuantizableLinear\n",
            "encoder.layers.encoder_layer_7.dropout: Dropout\n",
            "encoder.layers.encoder_layer_7.ln_2: LayerNorm\n",
            "encoder.layers.encoder_layer_7.mlp: MLPBlock\n",
            "encoder.layers.encoder_layer_7.mlp.0: Linear\n",
            "encoder.layers.encoder_layer_7.mlp.1: GELU\n",
            "encoder.layers.encoder_layer_7.mlp.2: Dropout\n",
            "encoder.layers.encoder_layer_7.mlp.3: Linear\n",
            "encoder.layers.encoder_layer_7.mlp.4: Dropout\n",
            "encoder.layers.encoder_layer_8: EncoderBlock\n",
            "encoder.layers.encoder_layer_8.ln_1: LayerNorm\n",
            "encoder.layers.encoder_layer_8.self_attention: MultiheadAttention\n",
            "encoder.layers.encoder_layer_8.self_attention.out_proj: NonDynamicallyQuantizableLinear\n",
            "encoder.layers.encoder_layer_8.dropout: Dropout\n",
            "encoder.layers.encoder_layer_8.ln_2: LayerNorm\n",
            "encoder.layers.encoder_layer_8.mlp: MLPBlock\n",
            "encoder.layers.encoder_layer_8.mlp.0: Linear\n",
            "encoder.layers.encoder_layer_8.mlp.1: GELU\n",
            "encoder.layers.encoder_layer_8.mlp.2: Dropout\n",
            "encoder.layers.encoder_layer_8.mlp.3: Linear\n",
            "encoder.layers.encoder_layer_8.mlp.4: Dropout\n",
            "encoder.layers.encoder_layer_9: EncoderBlock\n",
            "encoder.layers.encoder_layer_9.ln_1: LayerNorm\n",
            "encoder.layers.encoder_layer_9.self_attention: MultiheadAttention\n",
            "encoder.layers.encoder_layer_9.self_attention.out_proj: NonDynamicallyQuantizableLinear\n",
            "encoder.layers.encoder_layer_9.dropout: Dropout\n",
            "encoder.layers.encoder_layer_9.ln_2: LayerNorm\n",
            "encoder.layers.encoder_layer_9.mlp: MLPBlock\n",
            "encoder.layers.encoder_layer_9.mlp.0: Linear\n",
            "encoder.layers.encoder_layer_9.mlp.1: GELU\n",
            "encoder.layers.encoder_layer_9.mlp.2: Dropout\n",
            "encoder.layers.encoder_layer_9.mlp.3: Linear\n",
            "encoder.layers.encoder_layer_9.mlp.4: Dropout\n",
            "encoder.layers.encoder_layer_10: EncoderBlock\n",
            "encoder.layers.encoder_layer_10.ln_1: LayerNorm\n",
            "encoder.layers.encoder_layer_10.self_attention: MultiheadAttention\n",
            "encoder.layers.encoder_layer_10.self_attention.out_proj: NonDynamicallyQuantizableLinear\n",
            "encoder.layers.encoder_layer_10.dropout: Dropout\n",
            "encoder.layers.encoder_layer_10.ln_2: LayerNorm\n",
            "encoder.layers.encoder_layer_10.mlp: MLPBlock\n",
            "encoder.layers.encoder_layer_10.mlp.0: Linear\n",
            "encoder.layers.encoder_layer_10.mlp.1: GELU\n",
            "encoder.layers.encoder_layer_10.mlp.2: Dropout\n",
            "encoder.layers.encoder_layer_10.mlp.3: Linear\n",
            "encoder.layers.encoder_layer_10.mlp.4: Dropout\n",
            "encoder.layers.encoder_layer_11: EncoderBlock\n",
            "encoder.layers.encoder_layer_11.ln_1: LayerNorm\n",
            "encoder.layers.encoder_layer_11.self_attention: MultiheadAttention\n",
            "encoder.layers.encoder_layer_11.self_attention.out_proj: NonDynamicallyQuantizableLinear\n",
            "encoder.layers.encoder_layer_11.dropout: Dropout\n",
            "encoder.layers.encoder_layer_11.ln_2: LayerNorm\n",
            "encoder.layers.encoder_layer_11.mlp: MLPBlock\n",
            "encoder.layers.encoder_layer_11.mlp.0: Linear\n",
            "encoder.layers.encoder_layer_11.mlp.1: GELU\n",
            "encoder.layers.encoder_layer_11.mlp.2: Dropout\n",
            "encoder.layers.encoder_layer_11.mlp.3: Linear\n",
            "encoder.layers.encoder_layer_11.mlp.4: Dropout\n",
            "encoder.ln: LayerNorm\n",
            "heads: Sequential\n",
            "heads.head: Linear\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attr_path = ['encoder', 'layers', 'encoder_layer_0', 'self_attention']\n",
        "module = vit_model\n",
        "for attr in attr_path:\n",
        "    module = getattr(module, attr)\n",
        "self_attn_module = module"
      ],
      "metadata": {
        "id": "UkzuwBxaYIh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(self_attn_module.__class__.__name__)\n",
        "print(hasattr(self_attn_module, 'in_proj_weight'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJ32j9l5YcIS",
        "outputId": "7282f127-7629-4ee6-e304-75942ad03d26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultiheadAttention\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskGuidedAttention(nn.Module):\n",
        "    def __init__(self, in_features, out_features):\n",
        "        super(MaskGuidedAttention, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n",
        "        self.bias = nn.Parameter(torch.Tensor(out_features))\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.kaiming_uniform_(self.weight, a=np.sqrt(5))\n",
        "        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
        "        bound = 1 / np.sqrt(fan_in)\n",
        "        nn.init.uniform_(self.bias, -bound, bound)\n",
        "\n",
        "    def forward(self, image, mask):\n",
        "        mask = mask.unsqueeze(1)  # [16, 1, 224, 224] -> [16, 1, 1, 224, 224]\n",
        "        attention_weights = torch.softmax(image, dim=1)\n",
        "        mask = mask.broadcast_to(attention_weights.shape)\n",
        "        masked_attention_weights = attention_weights * mask\n",
        "        output = F.linear(masked_attention_weights, self.weight, self.bias)\n",
        "        return output"
      ],
      "metadata": {
        "id": "8xkJoHN952H4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "in_features = self_attn_module.out_proj.weight.shape[1]\n",
        "out_features = self_attn_module.out_proj.weight.shape[0]\n",
        "self_attn_module.out_proj = MaskGuidedAttention(in_features, out_features)"
      ],
      "metadata": {
        "id": "KCgliM8cYyAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, module in vit_model.named_modules():\n",
        "    if name != 'encoder.layer.0.SelfAttention':  # exclude the modified attention layer\n",
        "        for param in module.parameters():\n",
        "            param.requires_grad = False\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ajibc3vaeh84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TumorClassifier(nn.Module):\n",
        "    def __init__(self, num_classes, hidden_size=256):\n",
        "        super(TumorClassifier, self).__init__()\n",
        "        self.vit_model = vit_model\n",
        "        self.fc = nn.Linear(in_features=1000, out_features=num_classes)\n",
        "        self.fc.requires_grad=True\n",
        "        self.Softmax=nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, image):\n",
        "        image = self.vit_model(image)\n",
        "        return self.Softmax(self.fc(image))\n"
      ],
      "metadata": {
        "id": "P6gJ0Bqoioek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_attention_maps(image, output):\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
        "    ax[0].imshow(image.permute(1, 2, 0).cpu().numpy())\n",
        "    ax[0].set_title('Input Image')\n",
        "    ax[1].imshow(output.permute(1, 2, 0).cpu().numpy())\n",
        "    ax[1].set_title('Attention Map')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "hMTaMdwpLaHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier=TumorClassifier(num_classes=len(class_names))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
      ],
      "metadata": {
        "id": "t1cbndm7NiA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    classifier.to(device)\n",
        "    print('GPU!!!')\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print('CPU!!')"
      ],
      "metadata": {
        "id": "FvDnTNDk3uQJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "853cdb2c-5c73-4e15-9319-4d6579fb72ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "validation_accuracies = []\n",
        "validation_losses = []"
      ],
      "metadata": {
        "id": "lSV3uOisCBjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(60):\n",
        "  for batch in train_loader:\n",
        "    image ,mask, label_tensor= batch\n",
        "    image ,mask, label_tensor= image.to(device), mask.to(device), label_tensor.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    op= classifier(image)\n",
        "    loss=criterion(op,label_tensor)\n",
        "    train_losses.append(loss.item())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  print(f'Epoch{epoch+1},Loss{loss.item()}')"
      ],
      "metadata": {
        "id": "hY5YKR0lp_Z9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4019986-7125-4d29-8844-866c1487853b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch1,Loss1.0155643224716187\n",
            "Epoch2,Loss0.8478308916091919\n",
            "Epoch3,Loss0.681298017501831\n",
            "Epoch4,Loss0.6381515264511108\n",
            "Epoch5,Loss0.6167305707931519\n",
            "Epoch6,Loss0.6024188995361328\n",
            "Epoch7,Loss0.5928696393966675\n",
            "Epoch8,Loss0.5857340097427368\n",
            "Epoch9,Loss0.5798603296279907\n",
            "Epoch10,Loss0.5748457908630371\n",
            "Epoch11,Loss0.5706652402877808\n",
            "Epoch12,Loss0.5672688484191895\n",
            "Epoch13,Loss0.5645654201507568\n",
            "Epoch14,Loss0.562443733215332\n",
            "Epoch15,Loss0.5607826709747314\n",
            "Epoch16,Loss0.5594663023948669\n",
            "Epoch17,Loss0.5584028363227844\n",
            "Epoch18,Loss0.5575292110443115\n",
            "Epoch19,Loss0.5568017363548279\n",
            "Epoch20,Loss0.5561859607696533\n",
            "Epoch21,Loss0.5556532144546509\n",
            "Epoch22,Loss0.5551806092262268\n",
            "Epoch23,Loss0.5547520518302917\n",
            "Epoch24,Loss0.5543571710586548\n",
            "Epoch25,Loss0.5539911985397339\n",
            "Epoch26,Loss0.5536569952964783\n",
            "Epoch27,Loss0.5533654689788818\n",
            "Epoch28,Loss0.5531238913536072\n",
            "Epoch29,Loss0.5529271364212036\n",
            "Epoch30,Loss0.5527607202529907\n",
            "Epoch31,Loss0.5526075959205627\n",
            "Epoch32,Loss0.5524567365646362\n",
            "Epoch33,Loss0.5523185729980469\n",
            "Epoch34,Loss0.5522212982177734\n",
            "Epoch35,Loss0.5521647930145264\n",
            "Epoch36,Loss0.5521317720413208\n",
            "Epoch37,Loss0.5521113872528076\n",
            "Epoch38,Loss0.5520980358123779\n",
            "Epoch39,Loss0.5520889759063721\n",
            "Epoch40,Loss0.5520827770233154\n",
            "Epoch41,Loss0.5520788431167603\n",
            "Epoch42,Loss0.5520768165588379\n",
            "Epoch43,Loss0.5520762205123901\n",
            "Epoch44,Loss0.5520769357681274\n",
            "Epoch45,Loss0.5520786643028259\n",
            "Epoch46,Loss0.5520809888839722\n",
            "Epoch47,Loss0.5520836114883423\n",
            "Epoch48,Loss0.5520861148834229\n",
            "Epoch49,Loss0.5520880222320557\n",
            "Epoch50,Loss0.552088737487793\n",
            "Epoch51,Loss0.5520880818367004\n",
            "Epoch52,Loss0.552085816860199\n",
            "Epoch53,Loss0.5520820617675781\n",
            "Epoch54,Loss0.5520769357681274\n",
            "Epoch55,Loss0.552070677280426\n",
            "Epoch56,Loss0.5520633459091187\n",
            "Epoch57,Loss0.5520554780960083\n",
            "Epoch58,Loss0.5520470142364502\n",
            "Epoch59,Loss0.5520380139350891\n",
            "Epoch60,Loss0.5520287752151489\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score,confusion_matrix,roc_curve,auc\n",
        "from sklearn.metrics import precision_recall_curve,classification_report\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "def validate(classifier, val_loader, device):\n",
        "\n",
        "  classifier.eval()\n",
        "\n",
        "  y_true = []\n",
        "  y_pred = []\n",
        "  total_correct = 0\n",
        "  total_loss=0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "      images, mask, labels = batch\n",
        "      images, mask, labels = images.to(device), mask.to(device), labels.to(device)\n",
        "\n",
        "      op_v = classifier(images)\n",
        "      _, predicted = torch.max(op_v, 1)\n",
        "\n",
        "      total_correct += (predicted == labels).sum().item()\n",
        "      y_true.extend(labels.cpu().numpy())\n",
        "      y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "\n",
        "\n",
        "  return y_true, y_pred, total_correct\n",
        "\n",
        "\n",
        "y_true, y_pred, total_correct= validate(classifier, val_loader, device)\n",
        "\n",
        "visualize_attention_maps(image,output)\n",
        "\n",
        "\n",
        "\n",
        "accuracy = total_correct / len(y_true)\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "confusion_matrix = confusion_matrix(y_true, y_pred)\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(confusion_matrix, annot=True, cmap=\"Blues\",xticklabels=class_names,yticklabels=class_names)\n",
        "plt.xlabel(\"Predicted labels\")\n",
        "plt.ylabel(\"True labels\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "class_labels = range(len(np.unique(y_true)))\n",
        "\n",
        "report = classification_report(y_true, y_pred, target_names=class_labels, output_dict=True)\n",
        "print(\"\\n Classification Report:\")\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Overall Accuracy: {report['accuracy']:.4f}\")\n",
        "\n",
        "class_report_dict = {k: v for k, v in report.items() if k not in ['accuracy' ,'macro avg','weighted avg']}\n",
        "\n",
        "\n",
        "\n",
        "class_colors = {'Benign': 'blue', 'Malignant': 'red', 'Normal': 'green'}\n",
        "class_colors = {0: 'blue', 1: 'red', 2: 'green'}\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "for i, (class_name, metrics_dict) in enumerate(class_report_dict.items()):\n",
        "    plt.plot(range(3), [metrics_dict['precision'], metrics_dict['recall'], metrics_dict['f1-score']],\n",
        "                label=class_names, marker='o', color=class_colors[i])\n",
        "    for j, value in enumerate([metrics_dict['precision'], metrics_dict['recall'], metrics_dict['f1-score']]):\n",
        "        plt.text(j, value, f\"{value:.2f}\", ha=\"center\", va=\"bottom\", fontsize=10)\n",
        "plt.xlabel('Metric')\n",
        "plt.ylabel('Value')\n",
        "plt.title('Metrics for each class')\n",
        "plt.xticks(range(3), ['Precision', 'Recall', 'F1-Score'])  # Corrected xticks range\n",
        "plt.ylim([0.7,1])\n",
        "\n",
        "# Create a custom legend with colors and labels\n",
        "cls = {0:'Benign',1:'Normal',2:'Malignant'}\n",
        "legend_handles = [plt.Line2D([0], [0], marker='o', color='w', label=list(cls.values())[i],\n",
        "                             markerfacecolor=class_colors[i], markersize=10) for i in range(len(cls))]\n",
        "plt.legend(handles=legend_handles, loc='upper right', bbox_to_anchor=(1.05, 1))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dWhDvafHvZs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ih-2OpnlnIwk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}